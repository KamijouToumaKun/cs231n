鞍点（saddle point）和局部最优解（local minima）的问题
在高维空间中，局部最优解的可能性较小：一共上亿维，朝着每一个方向都无法更优化？
但是，鞍点：各个方向导数都是0，但在某些方向往上曲，在其他方向往下曲。还是较有可能的

*难道要计算二阶导数？
牛顿法（见图）：用二次函数而不是一次函数拟合函数的局部
好处：在原始版本中，直接跳到二次函数的极值点，而不用学习率了！！！
坏处：这个拟合也不一定好（尤其是噪声较大、非凸）；
计算代价太大了，需要求海森（Hessian）矩阵的逆
海森矩阵是N*N的，往往有N=1亿的量级；不仅计算难，内存都装不下
拟牛顿法（BFGS、L-BFGS）：求一个近似的逆

在梯度上加一个噪声？模拟退火：
以一定的概率来接受一个比单前解要差的解。通过这个随机因素使得算法有可能跳出这个局部最优解

参考资料：http://www.cnblogs.com/GeekDanny/p/9655597.html

SGD问题：随机梯度下降，每次计算的只是一个数据，并不是真正的由全部数据计算出的梯度
可以认为是BGD + 噪音，所以也可能导致走弯路
BGD：批梯度下降，真正的由全部数据计算出的梯度，但是计算量会很大
折中：mini-BGD，选择小批量数据进行梯度下降

解决方法：在SGD中加入动量（momentum）项
保持一个速度v，它从0开始，每次随时间衰减后（设摩擦因子为rho = 0.99）， += dx
更新方向不是梯度dx，而是速度向量vx：x -= learning_rate * vx
1）v[t+1] = rho * v[t] + d(x[t])
2）x[t+1] = x[t] + learning_rate * v[t+1]

想象一下，一个球滚下山崖的同时，不只计算它的位移方向，还考虑它的速度/惯性
vx是它原来的速度，受到摩擦影响而衰减，但仍然存在
现在，就算有dx = 0，也还有原来的vx，使它在鞍点或局部最优解处仍有速度跳出去
当然，也会使它走过最优点，然后再回头

也能解决zigzag问题：速度方向有相当大的一部分来自于原来方向
有保持原来方向的惯性，不至于zigzag

变体：nesterov momentum
同样保持一个速度v，它从0开始，也是每次随时间衰减（设摩擦因子为rho = 0.99）
但是，先让x -= learning_rate * vx
然后，计算这个位置的dx（不是最初的点处的梯度dx）
再让x -= learning_rate * dx
1）v[t+1] = rho * v[t] + d(x[t] + rho * v[t])
2）x[t+1] = x[t] + learning_rate * v[t+1]

Adagrad
用一个grad_squared变量一直累加dx multiply dx（每一维求平方）
每次x -= learning_rate * dx / (np.sqrt(grad_squared) + 1e-7)（每一维相除）
动量法是让dx朝着当前的惯性方向vx偏移，而Adagrad在训练中自动对learning_rate进行调整
想象一下，有一维的数据范围非常大，另一维非常小
那么np.sqrt(grad_squared)也会是相应的有一维大，另一维小
于是结果也是归一化了

同时，因为是累加，所以learning_rate也会随时间慢慢放缓
问题：局部最优解（local minima）的问题无法解决
变体：RMSprop（公式见图）
让learning_rate降低的速度没有那么快

SGD+动量的方法，速度一直保持，可能会绕弯路；
RMSProp，保持方向正确，但是速度没有那么快
	Adagrad不那么推荐

Adam：既考虑方向（Momentum），又考虑步伐（RMSProp）
在学习tensorflow例子的时候代码里面给的优化方案默认很多情况下都是直接用的AdamOptimizer优化

虽然对于这几个方法的设计原理，还是感觉很迷？？？……
结论：Adam好

――――――――――――――――――――――――――――――――――――

正则化笔记：
是一种扰乱措施，会使得训练的轮数变多，但是更健壮（robust），也避免了过拟合
符合条件的都属于一种正则化手段
1、批量归一化也有正则意义：一个数据每次被随机分到不同的batch里
而且它往往就够了。但是，如果过拟合了，还需要同时配上其他的方法

2、dropout技术，随机让每一层的一些神经元（以一定概率）死掉
什么叫做死掉？就是把一部分激活函数的输出临时改成0
一般在全连接层后使用，但有时也在卷积层之后使用！把整个channel置为0？？？
其实，python实现起来很简单（见图）
反向传播的时候不做任何改动？？？

为什么要这样做呢？举例
假如要判断对象是不是猫，有的神经元学习到了看耳朵，有的神经元学习到了看尾巴……
但是它们都没有学精，要结合起来才能够做出比较准确的判断
如果每次随机杀死其中的一些神经元，就能够逼迫它们都要能够独当一面

每次都随机杀死一些神经元，就相当于每次训练其的一个子神经网络
对于一个有N个节点的神经网络，有了dropout后，就可以看做是2^n个模型的集合了，但此时要训练的参数数目却是不变的，这就解脱了费时的问题。
（但是对于每一个子神经网络，训练得都不精啊？？？）

最后输出预测结果的时候，也是把每个子神经网络的判断结果做一个综合，然后输出
但是，子神经网络太多了
用采样 + 概率可以证明，只需要在激活层之后，乘以神经元的存活率p就可以了。
变体：在训练的时候，在激活层、dropout之后，除以p；
那么，输出预测结果时，在激活层之后，就什么也不用做了。
可以理解成，需要让占比p的神经元承担全部的训练效果，所以/=p将训练效果扩大。

3、data augmentation（数据增强）
对已有的数据做一些变换：比如图像对称、旋转、裁剪一部分、修改亮度对比度等；标签保持不变
有标准裁剪的方法，不少论文用这种方法建立扩展数据集，对自己的模型进行评估

4、drop connect
不是把激活层的输出临时变成0，而是把权重W中的一部分变成0

5、其他，还比较新的、不常用的方法，比如
Fractional Max Pooling
在CNN池化的时候，不是全部池化，而是随机挑选
Stochastic Depth
随机丢弃的是整个网络中的部分层！！！
输出预测结果的时候，再用上整个网络

――――――――――――――――――――――――――――――――――――

为什么会过拟合？数据不够
如何解决数据问题：迁移学习
迁移学习（Transfer learning）

以前在一个大的数据集（比如分1000类的ImageNet）上，训练出了一个CNN
1、如果新的数据集的差别跟原来的数据集差别不大
比如ImageNet里面有很多动植物分类，而新的数据集也是要分类动植物
1）现在，只有一个小的数据集合，要实现的分类器也只是分C（比如10）类
想把之前训练出的提取特征的能力，用到更小的数据集上面
方法：保持前面的权重，抹掉最后一个FC层的权重，重新对它进行随机初始化
当然，本来最后一个FC层的大小本来是4096*1000；现在也要改成：4096*C
然后，只训练最后一层；前面每一层的权重都冻结：因为它们的泛化能力已经很好了
冻结是指，可以不反向传播，或者可以把学习率调的很低很低，只有微调（finetune）
2）如果有一个较大的数据集合，可以多调整几层参数；冻结的层数减少（见图）

大部分CNN都不是从头训练的，而是在Google在ImageNet上预训练的模型的基础上精调的！！！
重新训练一个深层的网络
1）时间不够；2）数据不够；3）梯度扩散问题
如前所述，反向传播时，梯度随着深度向前而显著下降，导致前面网络参数贡献很小，更新速度慢。

3、如果新的数据集的差别跟原来的数据集差别很大
1）只有小的数据集合：放弃吧
2）有大的数据集合：再多调整（finetune）几层参数，可以是超过一半的层数

————————————————————————————————————

其他的解决过拟合的方法：model resembles
尝试多种随机初始化，训练出多个模型，然后共同投票表决出结果
这类似于随机森林？？？
