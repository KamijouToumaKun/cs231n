zero-centered：归一化成0均值
normalize: 0均值且尺度合适（比如-1～1）
方法：-= np.mean，/= np.std
这里是用np.mean(axis=0)计算出整个训练集（或者batch）的平均图像，比如AlexNet会这样做
也就是说，对数据的每一维求均值，当然不是对每个数据求均值
或者每个通道求一个平均图像，比如VGGNet
当然，测试集也要预处理，先减去训练集的平均图像（而不是测试集的）

有些归一化也不一定是0均值的，可以是0～1，或者0～255（图像处理中）
图像处理时往往只-= np.mean，而不/= np.std

把每一维都归一化成相同的水平，是防止梯度下降时zigzag（见图）
这种情况在更高维的空间中更加明显

*其他的方法：
PCA：主成分分析，降维。是一种使用最广泛的数据压缩算法。在PCA中，数据从原来的坐标系转换到新的坐标系，由数据本身决定。转换坐标系时，以方差最大的方向作为坐标轴方向，因为数据的最大方差给出了数据的最重要的信息。第一个新坐标轴选择的是原始数据中方差最大的方法，第二个新坐标轴选择的是与第一个新坐标轴正交且方差次大的方向。重复该过程，重复次数为原始数据的特征维数。

通过这种方式获得的新的坐标系，我们发现，大部分方差都包含在前面几个坐标轴中，后面的坐标轴所含的方差几乎为0,。于是，我们可以忽略余下的坐标轴，只保留前面的几个含有绝不部分方差的坐标轴。事实上，这样也就相当于只保留包含绝大部分方差的维度特征，而忽略包含方差几乎为0的特征维度，也就实现了对数据特征的降维处理。

*还有白化（whitening）。这些都是把方差也给归一化了，不一定必要。

归一化的好处：假如所有的数据离原点很远，而要用一根过原点的直线去分类它们
那么，只要直线偏一点（权值W的微小摄动），就会造成输出部分的极大摄动
造成大量的分类错误：过于敏感；而如果对数据做了平移，偏一点也没什么（见图）

——————————————————————————————

参数初始化
如果所有w都初始化成0，就算是ReLU，也不代表所有神经元都dead
但是，所有的神经元“将会做同样的事情”：同样的计算、同样的输出
这跟只有一个神经元没有区别

随机初始化，比如W = 0.01 * np.random.randn(D, H)，0.01表示高斯分布的标准差
但权重很小，对于深一些的网络可能产生问题：
1、前向传播的时候
假如输入数据的均值基本为0、标准差为1
在每一层都乘以一些随机小参数W之后
每一层的输出（activations）shrink了：均值更向0靠近，标准差也接近0了！！！
σ描述正态分布资料数据分布的离散程度，σ越大，数据分布越分散，σ越小，数据分布越集中。
见图，高斯分布逐渐变成一根棍：所有的输出基本都是0
2、反向传播的流程基本可以概括为：
1）误差/梯度 * W（忽略激活函数的导数部分），作为上一层的误差
2）于是，后面层的梯度 累乘 多个随机小参数W 得到前面层的梯度：前面层的梯度会很小。
X * 误差/梯度，加给W作为更新
前面层的梯度很小，后面层的X很小，都基本没有更新。
总之，输出小，梯度也小
（厘清一个概念：前向传播时，前面层是上游，后面层是下游；
反向传播时，后面层是上游，前面层是下游。上下游是根据传播方向来说的）

把权重变大？把标准差从0.01提高到1
新问题：饱和。现在要考虑导数部分了。
输出值比较大，再经过tanh之类的激活函数就通常在饱和区域。
(2010) Xavier initialization: W = np.random.randn(N_in, N_out) / np.sqrt(N_in)
这样可以使得输入的方差 = 输出的方差（=1），维持标准高斯分布
没有讲具体是怎么推导的
输入少，则分母小，算出来的权重大，维持方差的规模；
输入多，则分母大，算出来的权重小，维持方差的规模；

但是，如果你使用的是ReLU，它的饱和区域是负数，正数区间无饱和。（“一半的神经元被杀死”）
它导致的效果是，负数区域变成一根细柱子，正数区域还算正常。
但是，多层以后，最后也收缩成一根细柱子。
于是不能再像上面一样算了。有人说应该
W = np.random.randn(N_in, N_out) / np.sqrt(N_in/2)（因为“一半的神经元被杀死”）

批量归一化：loffe and Szegedy, 2015
方法：-= np.mean，/= np.std
如果原来数据是高斯分布，这就是在把它标准化；当然原来数据可能不是高斯分布，只是接近
归一化并不会使得数据原来的分布丢失，它只是做某种平移
要保留的结构更多的在于数据之间的联系，比如CNN中用filter去扫描
这样的空间结构是不会丢失的

但是不只是在最初的输入处这么做，而是每一层都这么做
批量归一化本身应该看成一个层：BN层；跟ReLU层、softmax层一样；有自己的梯度
对于全连接神经网络，一般插入到这个位置：FC->BN->activation
要让activation层的输入规范化。
还需要缩放和平移：左乘gamma，再加上平移因子beta
gamma和beta本身也是要学习的参数。
如果原来的数据好的话，那么gamma就趋向于std，beta就趋向于mean，数据接近于做了恒等变换
如果没有那么好，那么gamma和beta也能灵活地使得归一化的效果比较好，不会完全饱和
BN的梯度具体怎么算呢？？？

主要还是用在CNN上面，效果证实很好
据说用在强化学习（RL）上时，因为batch不够大，效果反而会变差！！！

只对每一层的输入输出进行变换，而不对权重W进行变换

有些人会强制让数据归一到（-1，1）（注意不是0～1）？？？

*同时，对数据本身做一定的变换，也是使得数据泛化；所以BN也有正则化的效果
因为正则化就是让模型不要过拟合，而让它能够泛化，即对其他样本也适用
