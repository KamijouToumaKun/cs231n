无监督学习（unsupervised learning）中的生成模型（generative model）
1、pixelRNN & pixelCNN
2、变分自编码器（variational autoencoders：VAE）
3、生成式对抗网络（generative adversarial networks：GAN）
……还有很多其他的模型（见图）

无监督学习是没有label的
好在，因为不需要label，数据量可以更多
而且，如果成功的话，就可以自己创造出合理的样本数据：作画、写文章、作曲……

1、k-means聚类。本来也不知道各个样本属于什么类，甚至不好说有哪些类
2、PCA降维。本来也不知道该怎么划分比较好
3、编码器，学习提取出数据的本质关键特征（也就压缩得更低维）。本来也说不清关键特征是什么
4、概率密度函数的估计（这应该是强化学习？）
但是，还是有办法评估任务的结果好坏的：比如聚类完以后，同类中两个样本之间的“相似性”可以量化
跟有监督学习的区别在于，学习本身要自己摸索着完成。
如何评估合理？P(x)够大。这里的x表示生成的图像/文章/音乐的整体
这个样本空间太大，样本太稀疏了。需要把x拆开来，评估诸xi，这里的xi表示像素点/字词等元素



显式密度模型
P(x) = πP(xi|x1x2...xi-1)，拆成链式的好处就是表示成了条件概率的连乘
同时把图像当成一个时序模型，可以分步治之
根据x1x2...xi-1xi，输出将会是像素的softmax loss值
找到使损失最小的xi，即P(xi|x1x2...xi-1)最大的xi（最大似然），作为当前生成的像素点
pixelRNN：按左上到右下的对角线，依次生产像素点：每个像素点由左边和上面的像素点生成
pixelCNN：从上到下从左到右地生成像素点，根据邻域中的已有部分来生成当前的像素点

当然，对序列初始值是敏感的。可以用0或者训练样本的平均值等不同的值来初始化，使输出多样化。

关于时序模型，我一直有一个疑惑：为了预测后面的值，要采用前面的值，但前面的值也是预测出来的
这样无监督地按链式顺序搞下去，会不会累积起某种误差、且越来越大？？？
马尔可夫过程，它拟合的是P(xi|x1x2...xi-1)
但是RNN，每个位置都有一个损失函数，也有一个总的损失函数、相应的梯度更新
马尔可夫有贪心解和全局最优解
RNN每一次给出的都是当前已知的全局最优解，而不是当前步最优解
目标就是最大化P(x)，即整幅图像的存在概率

——————————————————————————————————————————————————————

VAE采用另一种intractable（难于处理）的拆法
P(x) = ∫P(z)P(x|z)dz，z是隐（latent）变量
仍然难以估计，只能转而找出这个似然函数的下界，对其进行优化

VAE（变分自编码器）的思想由AE（自动编码器）而来
z就是低维特征。强制让高维数据x经过encoder，变得更低维
然后经过一个decoder，它跟encoder的结果是对称的，得到x^
目标是把原数据x还原回来，loss是x^与x的均方误差。这个过程并不需要label
训练完成以后，就可以直接用encoder提取出来的本质特征z作为有监督训练的输入
VAE更进一步，可以用来评估数据的好坏、生成新数据。

z是怎样分布，相对x可能更好统计/估计（或者说，估计的误差应该更小）
假设Z服从某些常见的分布（比如正态分布）
当然，正态分布也不止一种，需要抽样估计均值和协方差矩阵（因为是多维的），才能得到P(z)
同时，可以通过采样一些Z，然后根据Z来算对应的X的概率分布：P(x|z)
P(x|z)很复杂，但是已经被decoder学习到了
但是，怎么对于z积分/求和呢？P(x)仍然无法求解。（啊？连z的分布都知道了，却无法积分？？？）

转换思路，P(x) = P(z)P(x|z)/P(z|x)
似乎可行，P(z|x)被encoder学习到了？并没有，因为不知道x的分布种类，参数空间不同（？？？）
学习到的只能称为q(z|x)。但是就用它来近似P(z|x)了……
选择KL散度衡量两者的相近程度。参见https://www.jianshu.com/p/a750e666a8b7



另一篇文章给的解释不一样：https://spaces.ac.cn/archives/5253
其实，在整个VAE模型中，我们并没有去使用p(Z)（隐变量空间的分布）是正态分布的假设
给定一个真实样本Xk，我们假设专属于Xk的分布p(Z|Xk)（学名叫后验分布）
这个分布是（独立的、多元的）正态分布！

用2个网络，对于每一个样本xk，学习到p(Z|Xk)的均值和方差
根据p(Z|Xk)进行抽样得到z
z经过decoder得到x^

VAE本质上就是在我们常规的自编码器的基础上
对encoder的结果（在VAE中对应着计算均值的网络）加上了“高斯噪声”
使得结果decoder能够对噪声有鲁棒性。这也是一个对抗的过程
结果1、直接从不同的z出发，生成新的数据（而且可以看出，新的数据是随着z进行连续变化的）
2、可以给图像做去噪等修复

还是不懂……？？？

——————————————————————————————————————————————————————

隐式密度模型：GAN，目前生成效果最好的模型
VAE生成的 image 是希望和 input 越相似越好，但是 model 是如何来衡量这个相似呢？model 会计算一个 loss，采用的大多是 MSE，即每一个像素上的均方差。loss 小真的表示相似嘛？

GAN不期望拟合一个概率函数，而是模拟一个两方进行博弈的过程，最后两方同时提高
输入一个随机噪声，通过生成器网络（GAN），输出。它的目标是让自己输出的图像骗过判别器网络
判别器网络（discriminator network），它的目标是分辨出真实图像和GAN生成的图像
两者共同训练，目标函数是最大最小博弈公式（minimax game formulation）

或者说，这是一个最大最小博弈（minimax game），两个网络轮流训练：
首先，有一个一代的 generator，它能随机生成一些很差的图片，
然后有一个一代的 discriminator，它能准确的把生成的图片，和真实的图片分类，
接着，开始训练出二代的 generator，它能生成稍好一点的图片，能够让一代的 discriminator 认为这些生成的图片是真实的图片。
然后会训练出一个二代的 discriminator，它能准确的识别出真实的图片，和二代 generator 生成的图片。
以此类推，会有三代，四代。。。n 代的 generator 和 discriminator，
最后 discriminator 无法分辨生成的图片和真实图片，这个网络就拟合了。

看作业里面，generator并没有用到什么概率公式，也没有使用CNN结构？？？
就是一个三层的全连接网络，激活函数分别是ReLU、ReLU、tanH（最后保证变换到-1～1）
generator 想最小化假的得分（但同时不追求真分类的得分最大化？？？）
discriminator 也是全连接的，用到了leaky ReLU；想加强分类能力

使用tf.nn.sigmoid_cross_entropy_with_logits，sigmoid的交叉熵
交叉熵作为loss有很多应用场景，最大的好处是可以避免梯度消散
loss的计算：
logits_real保存每一张图片是真的得分, logits_fake保存每一张图片是假的得分
用sigmoid转换为可能性（而并不由softmax来做归一化）
1、generator loss = -tf.reduce_mean(tf.log(normal_fake_pro))
2、discriminator loss = -tf.reduce_mean(tf.log(normal_real_pro)) - tf.reduce_mean(tf.log(1 - normal_fake_pro)) 

另外，梯度下降使用了AdamOptimizer，反正tensorflow方便，不用白不用
希望看到G_loss 逐渐变得足够小，而D_loss 逐渐变大
Epoch: 0, D: 1.336, G:0.8016
Epoch: 1, D: 1.132, G:1.172
...
Epoch: 9, D: 1.239, G:0.7269
Final images
而看别人的效果：Iter: 4200, D: 1.336, G:0.7591 
说明这就是极限了
生成效果并不大真实，我想是因为分类器没有使用CNN结构

换一套损失函数：Least Squares GAN
奇怪的是，均方误差反而比交叉熵模型更新？
1、generator loss = scores_fake与1之间的均方误差
2、discriminator loss = [scores_real, scores_fake]与[1, 0]之间的均方误差

Epoch: 0, D: 0.6009, G:0.5689
Epoch: 1, D: 0.1483, G:0.2733
...
Epoch: 8, D: 0.2279, G:0.1777
Epoch: 9, D: 0.2163, G:0.2102
Final images
而看别人的效果：Iter: 4250, D: 0.2131, G:0.1725 
说明这就是极限了
生成效果还是不大真实，绝对是因为没有使用CNN结构

终于给分类器加上了CNN结构：Deep Convolutional GANs
当然，也就慢的多了
Epoch: 0, D: 0.9508, G:1.246

Inline Question指出，交替最小化同一目标（minimax函数），可能是一个棘手的问题
虽然我无法回答给出正解，但我有一些自己的想法
按说，交替进行梯度下降，是一个只看当前的贪心策略
而棋类游戏的最大最小博弈，要向前看很多步，形成一棵搜索树，才能更接近全局的最优解
也就是说，每一步尝试加不同的正则项/动量等，然后进行梯度下降
这相当于每一步要尝试不同的选择，也要去尝试那些非贪心的选择
但是，棋类本身，每一个动作和状态的转换都是很明晰的，尚且承受不住这样的搜索
更何况我这样，每尝试一个新动作都是做一次新的梯度下降。这样做的时间和空间复杂度绝对爆炸。
就算改用强化学习也撑不住啊。