可解释性：interpretable
如何直观地感受到深度网络的每一层训练的怎么样了？
如何能理解到，每一个中间层的意义：它是在寻找什么特征？
————如何解释神经网络的训练：它并不真是一个黑箱？让人类能信服它的训练？

filter可视化
filter作为图像表现出来
比如，对于3通道的图像，用64组11*11的filter去做卷积
对于训练出来的filter，可以可视化为64个图像
每个图像为11*11*3：把每组的3个也按照RGB的方式叠成一个图像
之前没有搞懂的是，这些可视化的图像说明什么呢？

假如图像有些地方颜色浅（接近白），那说明，它是在判断原图在这些位置是否也有这些形状
如果原图也有，那么这些大的权重就会让原图的输出很大

第一层的filter的颜色浅的地方一般都是条纹状的
也就是说，第一层一般是在寻找图像的有向边
当然，也能看到有一些filter大部分都是白色的：没有学习到什么关键之处
是的，这些filter之间是不大相同的
但是这64组filter的地位是平等的，为什么它们能学习到不同的东西，而不是趋同呢？？？

中间层的filter的可解释性要差一些
因为，输入通道数一般不再是3，比如第二层是64
64张叠在一起？叠不起来，只能把64张灰度图排成一排
然后看一看，是不是有一些灰度图的形状是我们想要的（比如人脸）
还是不方便啊
1、不能直接可视化filter，只能看哪些图像对它的胃口：
Visualize patches that maximally activate neurons
给定某一层的某一个通道
把很多图像都输入网络，看哪些图像出来的得分比较高：最能激活这层的神经元
人为地归纳这些图像的共同点：是圆形/方形/边缘……

37～38min处的另一种思路：人工地计算，什么样的图像能让层输出值最大
也就是说，神经网络认为，什么图像最符合“狗”/“花”……的概念。比较好的结果是：
很多只狗、很多朵花的轮廓堆在一张图像上的样子（但色彩总是很迷……像彩虹一样五彩渐变）
同时，使用正则项（比如图像矩阵的L2范数），让生成的图像不要过拟合、显得更自然
这同样是使用梯度下降（这里是上升）来实现，但是权重是固定的（fix）
更新的是x：计算出的dx最后不是为了作为参数而已，要真的加在最初的x上

2、Occursion Experiment（排除实验）
尝试用滑动窗口遮掉图像的一部分（改成全图的平均值）
看哪一部分/哪些像素最能干扰网络的判断，这可以画成一张热力图（见图）
最能干扰判断 = 正确分类的概率下降最多/正确分类的得分下降最多（saliency maps）
这已经接近于对抗网络这种概念

一般来说，热力图的高值部分应该就是要判断的物体本身
这是不是用来做segmentation的一种新思路呢？经试验，效果不太好

最后的全连接层的可视化
全连接层的输入：经过多次卷积得到的，一个多通道的小图像，被拉成的一维向量
回想kNN算法：对于一张图像，在pixel space上寻找其top-k的最近邻图像
现在改成：对于一张图像，在最后一层的feature space上寻找其top-k的最近邻图像
看看它们是不是的确是一类图像（语义内容相同）
注意，它们在pixel space上可能差别很大
比如一张图左边是草地，右边是大象；另一张图左边是大象，右边是草地
还可能一个是草地一个是泥地；在pixel space上差别很大，但是的确都是大象
如果这两张图在feature space上很临近，那说明深度网络训练得好！

两个利于人为解释的loss函数：
1、Contrastive Loss (对比损失)
https://blog.csdn.net/autocyz/article/details/53149760 
这种损失函数最初来源于Yann LeCun的Dimensionality Reduction by Learning an Invariant Mapping，主要是用在降维中，即本来相似的样本，在经过降维（特征提取）后，在特征空间中，两个样本仍旧相似；而原本不相似的样本，在经过降维后，在特征空间中，两个样本仍旧不相似。
不过现在我们要的是，原来不相似的现在也可能相似啊。
唯一的确认方法是目测？
2、Triplet loss，通常用来做人脸识别(认证)
每次的输入由三元组成：Anchor、Negative、Positive。这个loss希望学习后
同类样本的positive样本更靠近Anchor，而不同类的样本Negative则远离Anchor。

高维数据集的可视化
回想PCA。它可以应用于图像压缩，其基本思想就是设法提取数据的主成分
本质就是把高维数据降维到低维进行表示
每一个点不只标成一个黑点，可以放上缩小的原图，或者它的语义：花、狗
*还有一种方法：t-SNE（t-分布邻域嵌入）降维（dimensionality reduction）
总之，把一堆高维数据点降到二维/三维，就能将它可视化
于是能看到，数据集会呈现聚类分布：这团对应花，那团对应狗……

如果最后一层全连接层的输入也呈明显的聚类分布，那说明之前的卷积做的比较好
否则最后一层全连接实在难以做好分类：独木难支

*guided backprop：在反向传播的时候也用ReLU处理一下。
听起来很玄乎……

对抗（adversarial）网络：拿一张大象的图片，然后训练一个网络
让它修改图像，尽量最大化这张图像的考拉的分值（fooling image）
loss应该就是考拉的分值 + 跟原来图像的差值（作为正则项）吧
我们希望得到的图像明显不再像大象了，而有了考拉的耳朵/尾巴
但是，结果看起来明明还是大象（看起来只加了一点微小的噪声），就判断成考拉了！

37～38min处的进一步说明：DeepDream
在2013年“Visualizing and Understanding Convolutional Neural Networks”这篇文章提出了使用梯度上升的方法可视化网络每一层的特征
即用一张噪声图像输入网络，反向更新的时候不更新网络权重，而是更新初始图像的像素值，以这种“训练图像”的方式可视化网络。deepdream正是以此为基础。
https://blog.csdn.net/accepthjp/article/details/77882814 有一些效果图

如果初始时不用噪声输入网络呢？
有一个网络学习了分类猫和狗的任务，给这个网络一张云的图像，这朵云可能比较像狗，那么机器提取的特征可能也会像狗。假设对应一个特征[0.6, 0.4], 0.6表示为狗的概率，0.4表示为猫的概率，那么采用L2范数可以很好达到放大特征的效果。对于这样一个特征，用L2 = x1^2 + x2^2作为最大化的目标函数
迭代的轮数越多，x1越大，x2越小，所以图像就会越来越像狗：云 + 狗。

也可以不最大化L2，而是最小化特征向量之间的距离？？？

如果从较前面的层开始反向传播呢？反应出的是一些纹理之类的东西：
风格迁移、纹理合成（texture synthesis）
在没有神经网络的时候就有很多依据邻域来合成纹理的算法
用深度网络处理一张纹理图，提取出特征：H*W*C
看作H*W个C维向量，然后计算Gram矩阵
Gram矩阵就是用来度量图像的风格纹理的（原理不用管了……？？？）
该算法旨在生成与目标图像具有相同gram矩阵的图像
这里计算的损失是生成图像与目标图像各层gram矩阵的加权L2距离

从每一层都可以开始做feature inversion，看原图进行到这一层时
根据与这一层的feature之间的误差，从空白图/噪声图反向恢复原图，得到的图像跟原图差距大不大
见图，越深的层应该越抛弃掉一些图像的细节，但是保留图像的基本语义信息。

多尺度处理（multiscale processing）
从一个小图像开始，不仅放大特征，而且开始将图像放大
最后得到疯狂的效果（见图）