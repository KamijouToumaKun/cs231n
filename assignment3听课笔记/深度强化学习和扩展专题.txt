关于强化学习，提三点：
1、MDP（马尔可夫决策）
2、Q-learning & Actor-Critic
3、policy gradient（策略梯度）

还是等到David Silver的课上去系统地学习吧
还是Q(s,a)的问题，矩阵太大、无法直接求解
RL的方法分成两部分：
Policy-based：学习到一个Actor
Value-based：学习到一个Critic
将Actor与Critic结合起来是目前最有效果的做法。
深度强化学习，就是用神经网络来拟合Q(s,a)

————————————————————————————————————————————

对抗（adversarial）网络：这里做一个了解即可。不深入讨论。

之前提到过。拿一张大象的图片，然后训练一个网络
让它修改图像，尽量最大化这张图像的考拉的分值（fooling image）
loss应该就是考拉的分值 + 跟原来图像的差值（作为正则项）吧
我们希望得到的图像明显不再像大象了，而有了考拉的耳朵/尾巴
但是，结果看起来明明还是大象（看起来只加了一点微小的噪声），就判断成考拉了！

更可怕的是，原图对大象的置信度，还不如改造图对考拉的置信度！
而且，各个AI模型都不能避免这样的攻击：不一定是深度学习，还有机器学习
SVM；决策树；KNN

1、本质在于过拟合吗？那么，对抗样本应该属于某种BUG，比较少见
但是，对抗样本又很容易训练出来，任意给定一张图片和一个目标分类都可以！
用原图减去对抗样本，差看似一个随机噪声，却是有规律的
把它加到另一张正常图片上，也会引起往该目标分类的误分类：这个误导是定向的
2、本质在于模型的缺陷吗？
对抗样本对于分类的误导是跨模型的！
所以：
3、网络判断是不是大象/考拉的，是根据某种无法捉摸的东西
这是一种系统性效应（systematic effect）
它又并不是根据耳朵/尾巴等东西来判断的。反倒更像是欠拟合
是不是深度网络的本质还是太线性了？
激活函数：常见的LSTM、ReLU和maxout网络也是接近分段线性的
所以拟合出来的函数也只能接近分段线性？

把图像的L2范数变化同样的程度，但是变化的方式可以不同（见图）
各种对抗样本：有的加上一部分，有的去掉一部分，这些都是根据人的思维来进行误导
但是，还是FGSM（Fast Gradient Sign Method）最快：
在梯度方向上添加增量，来诱导网络进行误分类

当然，这需要知道模型内部是怎么样的
黑盒测试把模型当做黑盒，只能输入样本获得预测结果
白盒在黑盒的基础上还可以获取模型的参数、梯度等信息。