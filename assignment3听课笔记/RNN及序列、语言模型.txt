循环神经网络RNN

CNN本身的参数很少，坏处在于输出的通道太多了
于是用于分类的那些全连接层，参数太多、计算量太大：上千万个参数

AlexNet的结构
	CONV+MAXPOOL+BN
	CONV+MAXPOOL+BN
	CONV
	CONV
	CONV+MAXPOOL
	FC 256*6*6 -> 4096
	FC 4096 -> 4096
	FC+SOFTMAX 4096 -> 1000

GoogLeNet（19层和22层，内嵌了LeNet的名字）
ResNet有152层；据算可以达到1000层
它们没有在最后采用全连接层分类，而是使用平均池化。这样的架构更好

RNN关心的是1、序列问题，2、可以处理任意长的序列（一般是文字/视频）
同时，输出也可能是不定长的
一个序列结束时，补上一个特殊字符<END>作为标记（不能是句号之类的）

举例：（见图one and many）
one-to-many：输入是一幅图，输出是对其的文字描述
	注意，不一定要真输入一个图像，可以输入一个encoder得到的图像feature即可
	比如，把AlexNet的最后一层全连接去掉，这就是一个encoder；拿到的输出就是feature！
many-to-one：输入是一段文字，输出是其中的情感属性/其他方面上的分类
many-to-many：
	1、机器翻译。输入输出的长度都不确定，而且不同。
	输出的时机也有不同。翻译应该是要读完整句话，才开始输出
	这个过程其实是一个encoder（many-to-one）+decoder（one-to-many）
	得到的one的中间结果，就是整个句子的含义
	2、输入输出的长度相同，而且可以同步输出。
	比如对视频的每一帧都进行分类，只是分类的过程跟上下文有关。
有些本来不是sequential processing的，也可以用RNN
	比如把图像的像素点当成有序序列

序列的任意部分都用同一个单元处理，它包含权重W和隐状态h（都是共用的）
权重w在训练的时候确定下来。测试时，w不变；
但是处理序列的当前x[t]时，单元内部的h[t-1]会根据x[t]和w而变化，变为h[t]（新的隐状态）
h[t] = tanh(W[hh]*h[t-1] + W[xh]*x[t])。特别的，可以把W[hh]和W[xh]合并
之后会解释，为什么这里用tanh激活
中间过程得到的诸h[t]也是有意义的。可以再训练一个网络对它们做处理，得到y[t]序列
最后再对y[t]序列做处理，得到总的输出

损失函数：
many-to-many：见图
对每一个时刻的预测值y[t]，都有一个相对应的真实的分类结果（one-hot的）
这甚至可以是无监督的：比如，输入一篇文章，拟合的就是下一个单词是什么
然后每一个时刻都有一个loss可以回传。这里可以使用softmax loss
当序列输入完毕后，得到回传的所有梯度，加起来一起对于W进行更新
one-to-many：见图
many-to-one：更简单了，最后的loss来源只有一处了（见图）

语言模型：
在每一个时刻，输入是一个字母或者单词，它们可以用one-hot编码
输出也是一个多维向量了，毕竟本质是一个分类问题
而且，也能保留其他概率大的可能，使得输出不唯一
比如，RNN拟合的就是下一个单词是什么
那么，从一个随机多维向量开始，得到概率最高/较高的下一个单词
然后将其作为下一次的输入……
注意，还是输入one-hot向量，而不是直接输入上次的softmax得分，因为要和训练阶段保持一致才有意义
这样就可以生成一篇新的文章————虽然没有教给它句子结构，但它能学会！

one-hot的意义：让维数比较多（比如，一共有10000个单词，那么就需要10000维）
也要保持稀疏，这样可以简化很多计算
另一种选择就是通过学习，对每个单词进行编码，feature的维数比较少而不是one-hot的

之前说到，当序列输入完毕后，得到回传的所有梯度，加起来一起对于W进行更新
如果序列特别长，这样会很低效
可以尝试truncated的回传：每得到100个时刻的loss，就进行一次回传
然后再重新等100个时刻：它们跟上100个不重叠

学习了莎士比亚的作品数据集，它能理解小标题代表某角色要开始说话了；换行符表示换行
也能模拟出莎士比亚的语言风格（Shakespeare-esque）
如果学习用Letax写出的数学教科书（公式、下标、有向图等格式都可以用纯文本表示出来）
那么模型也可以编出一本看起来挺像教科书的东西，知道插入标题、插入公式，引用引理……
还知道写“证明略”（Proof. Omitted)

学会写C代码，保持不错的代码风格，甚至写注释（尽管注释没有什么用）

cell的意义是可以解释的
1、发现有的cell在寻找引号。它的输出一直很大（红色）
直到出现一个前引号时，输出变小（变为蓝色）
当后引号补全以后，输出又变大
2、发现有的cell在等待回车。它的输出本来很小（蓝色），逐渐变大
直到出现一个回车时，输出重新变小（变为蓝色）
……

————————————————————————————————————————————————————

进阶的模型：LSTM，添加了注意力（Attention）机制
RNN一直保留着之前所有输入的影响，但其实，最近的输入对当前输出的影响比较大，远处的比较少
不应该一直保留着对于初始输入的关注：“分心”

把图像压缩成：不是一个向量，而是多个向量（构成一个二维阵列）
由此，需要最后保留一层作为CNN，这样的encoder才能输出二维阵列
把二维阵列作为RNN的输入

GRU等算法

LSTM的遗忘门之类的东西好复杂啊？？？而且似乎很玄学。